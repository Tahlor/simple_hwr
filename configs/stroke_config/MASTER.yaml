## TO DO IMMEDIATELY:
# DEBUG TEST/TRAIN LOSS
# RELATIVE COORDS CONFIG

# Load model
TESTING: false # this is for debugging, do it on a small dataset
load_path: false # ./results/baseline/20190622_155031-lr_0.001_bs_16_warp_False_arch_basic_encoder/baseline_model.pt # or false
test_only: false # Do not train
offline_pred: false
logging: info
gpu_if_available: true
load_optimizer: false # load LR and per-parameter LR's from saved state
reset_LR: true # reset the global LR if loading optimizer

# General
output_folder: ./results
epochs_to_run: 1000
update_freq: 100 # print out updates, update graphs etc. after this many updates
save_freq: 1     # save the model every X epochs
test_freq: 10    # test, save the graphs
use_visdom: true
debug: off

# Training data
#dataset_folder: online_coordinate_data/8_stroke_vSmall_16
#dataset_folder: online_coordinate_data/8_stroke_vFull
data_root_fsl: ../hw_data/strokes
data_root_local: data
dataset_folder: MAX_stroke_vNORMAL_TRAINING_TESTFull # normal testing
dataset_folder: online_coordinate_data/MAX_stroke_vFull
dataset_folder: online_coordinate_data/MAX_stroke_vTEST_AUGMENTFull
dataset_folder: online_coordinate_data/MAX_stroke_vlargeTrnSetFull

dataset:
  img_height: 61
  include_synthetic: true
  num_of_channels: 1
  image_prep: pil_with_distortion # keywords: distortion (anywhere), pil (starts with), no_warp (anywhere)
  #gt_format - specified elsewhere
  #batch_size - specified elsewhere
  adapted_gt_path: null
  resample: false

# LR schedule
learning_rate: 5e-4          # LR
scheduler_step: 25         # Every X steps, multiply LR by gamma
scheduler_gamma: .95          # LR decay rate

## Loss options:
  # Based on width of image, determine how many outputs there should be
    # batches make predictions square, ONLY evaluate based on the expected GT desired_num_of_strokes
  # DTW - have as many GTs as you want; bound alignments somehow?
  # (Old option: resample the GTs after the prediction is known)
  # (Future option: with attention, have the GTs to be just be sampled regularly)

test_size: null
train_size: null
batch_size: 32

## GTs
# All options include:
  # x pos, y pos
  # sos - 'is start of stroke' (1's for yes, 0's for no)
  # sos_interp - 'is start of stroke' interpolated
  # sos_interp_dist - 'is start of stroke' interpolated
  # stroke_number - 'is start of stroke' interpolated
  # eos - 'is start of stroke' interpolated

# interpolated_sos: interpolated # normal: use 1's for starts; interpolated: start is a "0" and increases from there based on distance of stroke
gt_format: # if relative etc., specify that here; e.g., opts:rel
  - x
  - y
  - stroke_number
  - eos

# NOT IMPLEMENTED
gt_opts:
  - null
  - null
  - null
  - null

# stroke_number is the cumsum of sos; gt_format, gt_opts, pred_opts: "sos", "cumsum", "cumsum" = "start_stroke", "null", "cumsum"
# E.g. cumsum will predict RELATIVE positions; if "null" is specified for gt, it will compare to absolute values

pred_opts: # if relative etc., specify that here; e.g., opts:rel opts:cumsum etc.
  - null # CUMSUM HAPPENS BEFORE THE LOSS FUNCTION
  - null
  - cumsum sigmoid # THIS IS APPLIED AFTER THE LOSS
  - sigmoid # THIS IS APPLIED AFTER

## Loss function
# l1, dtw, ssl, cross_entropy
# can also add activation functions here NOT IMPLEMENTED
# LOSS FNs WITH THE SAME NAME WILL BE CONSIDERED THE SAME!


loss_fns:
  - name: l1_with_stroke_numbers
    coef: 1
    gts:
      - x
      - y
      - stroke_number
    subcoef: 1,1,.1
  - name: cross_entropy
    coef: 1
    gts:
      - eos
    activation: sigmoid

loss_fns2:
  - name: dtw # dtw_sos_eos, dtw_reverse
    method: both   # both, normal, "align_to_gt", "align_to_pred"
    coef: 1
    gamma: 1       # soft DTW
    reverse: false # soft DTW
    barron: true # use Barron
    low_level_dtw_alg: invert # use the invert DTW
    gts:
      - x
      - y
      - stroke_number
    dtw_mapping_basis:
      - x
      - y
    cross_entropy_indices: # BCEWITHLOGITS TAKEN DURING LOSS, NO SUBCOEF
      - sos
    relativefy_cross_entropy: true # this will take cross_entropy_indices GTS ONLY and relativefy it; this will find the "true" first stroke point
    subcoef: 1,1,.1 # ONLY APPLIES TO GT-index list
  - name: cross_entropy
    coef: .01
    gts:
      - eos
    activation: sigmoid
  - name: nnloss # dtw_sos_eos, dtw_reverse
    method: both   # both, normal, "align_to_gt", "align_to_pred"
    coef: 1
    gamma: 1       # soft DTW
    reverse: false
    barron: true # use Barron
    low_level_dtw_alg: invert # use the invert DTW - don't use this
  - name: dtw_adaptive
    coef: 1
    no_swapping: true # disable swapping
    gts:
      - x
      - y
    dtw_mapping_basis:
      - x
      - y
    cross_entropy_indices: # SIGMOID TAKEN DURING LOSS
      - stroke_number
    relativefy_cross_entropy_gt: true #
    window_size: 40
    barron: false



# Always report these, but don't include in backpropagation
#loss_fns_to_report:
#  - name: l2_coords
#    gts:
#      - x
#      - y
#  - name: l1_coords
#    gts:
#      - x
#      - y

convolve_func: cumsum # or conv_weight, conv_window, cumsum
cumsum_window_size: 21 # only for the conv_window and conv_weight

first_loss_epochs: 2
training_nn_loss: false # calculate nearest neighbor loss on every training instance; <- this is basically just DTW loss, without the time constraint? really only useful for offline images with pixels etc.
# test_nn_loss: true # calculate nearest neighbor loss on every test instance

## CNN
cnn_type: default64 # default64; CNN output width similar to input width; default: CNN output is like 1/4 the input width

# CoordConv
  # x-scaled from -1 to 1
  # x-scaled to be same scale as y
coordconv: true
coordconv_method: y_abs
coordconv_0_center: false

# Visdom
visdom_port: 9001

## Default
# L1, DTW, SSL etc.
## Try Barron loss
## Try SoftDTW
## Need to compare NN loss probably
