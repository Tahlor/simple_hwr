
# Load model
TESTING: false # this is for debugging, do it on a small dataset
use_visdom: false
load_path: false # ./results/baseline/20190622_155031-lr_0.001_bs_16_warp_False_arch_basic_encoder/baseline_model.pt # or false
test_only: false # Do not train
offline_pred: false
logging: info
gpu_if_available: true
load_optimizer: false # load LR and per-parameter LR's from saved state
reset_LR: true # reset the global LR if loading optimizer

model_definition:
  model_name: Renderer
  input_vocab_size: 3
  device: cuda
  cnn_type: default64

# Stroke model
stroke_model_pt_override: "./renderer/stroke_models/RESUME_model.pt"
stroke_model_config: "./renderer/stroke_models/RESUME.yaml"

# General
output_folder: ./results
epochs_to_run: 1000
update_freq: 100 # print out updates, update graphs etc. after this many updates
save_freq: 1     # save the model every X epochs
test_freq: 1    # test, save the graphs
debug: off

# Training data
#dataset_folder: online_coordinate_data/8_stroke_vSmall_16
#dataset_folder: online_coordinate_data/8_stroke_vFull
data_root_fsl: ../hw_data/strokes
data_root_local: data
#dataset_folder: online_coordinate_data/3_stroke_64_v2
dataset_folder: online_coordinate_data/MAX_stroke_vlargeTrnSetFull

# MAKE THIS BETTER
trainer_args:
  loss_type: mse
loss_fns:
  - name: ImageL2 # dtw_sos_eos, dtw_reverse
    gts: # doesn't actually do anything except make syntax checkers happy
      - x
      - y


dataset:
  img_height: 61
  include_synthetic: false
  num_of_channels: 1
  image_prep: pil # keywords: distortion (anywhere), pil (starts with), no_warp (anywhere)
  adapted_gt_path: null

# LR schedule
learning_rate: 5e-4          # LR
scheduler_step: 25         # Every X steps, multiply LR by gamma
scheduler_gamma: .95          # LR decay rate

## Loss options:
  # Based on width of image, determine how many outputs there should be
    # batches make predictions square, ONLY evaluate based on the expected GT desired_num_of_strokes
  # DTW - have as many GTs as you want; bound alignments somehow?
  # (Old option: resample the GTs after the prediction is known)
  # (Future option: with attention, have the GTs to be just be sampled regularly)

test_size: null
train_size: null
batch_size: 16

## GTs
# All options include:
  # x pos, y pos
  # sos - 'is start of stroke' (1's for yes, 0's for no)
  # sos_interp - 'is start of stroke' interpolated
  # sos_interp_dist - 'is start of stroke' interpolated
  # stroke_number - 'is start of stroke' interpolated
  # eos - 'is start of stroke' interpolated

# interpolated_sos: interpolated # normal: use 1's for starts; interpolated: start is a "0" and increases from there based on distance of stroke
gt_format: # if relative etc., specify that here; e.g., opts:rel
  - x
  - y
  - stroke_number

stroke_input_opts: # these are now the inputs
  - rel  # input x-coordinates should be relative!
  - null
  - rel # these should be SOS etc.

gt_opts: # These are always nothing; always ABSOLUTE and either SOS or stroke_number (depending on above)
  - null
  - null
  - null

# stroke_number is the cumsum of sos; gt_format, gt_opts, pred_opts: "sos", "cumsum", "cumsum" = "start_stroke", "null", "cumsum"
# E.g. cumsum will predict RELATIVE positions; if "null" is specified for gt, it will compare to absolute values
pred_opts: # if relative etc., specify that here; e.g., opts:rel opts:cumsum etc.
  - null
  - null
  - cumsum sigmoid # THIS IS APPLIED AFTER THE LOSS

## Loss function
# l1, dtw, ssl, cross_entropy
# can also add activation functions here NOT IMPLEMENTED
# LOSS FNs WITH THE SAME NAME WILL BE CONSIDERED THE SAME!

first_loss_epochs: 0

## CNN
cnn_type: default64 # default64; CNN output width similar to input width; default: CNN output is like 1/4 the input width

# CoordConv
  # x-scaled from -1 to 1
  # x-scaled to be same scale as y
coordconv: true
coordconv_method: y_abs
coordconv_0_center: false

# Visdom
visdom_port: 9001
